---
title:
    AI safety reading group
description:
    “For thousands of years men dreamed of pacts with demons.
    Only now are such things possible.”
---
    
<!--
`You are worse than a fool,' Michéle said, getting to her
feet, the pistol in her hand.  `You have no care for your species. 
For thousands of years men dreamed of pacts with demons. 
Only now are such things possible.  And what would you be
paid with?  What would your price be, for aiding this thing to
free itself and grow?'

---William Gibson, <i>Neuromancer</i>
-->

AI safety reading group
=======================

Weekly discussions of readings on technical and philosophical topics in
AI safety.

AI Safety is the field trying to figure out how to stop AI systems from
breaking the world, and in particular, trying to do so before they break
the world.
Readings will span from potential issues arising from future advanced
AI systems, to technical topics in AI control, to present-day issues.

Seminar information:

* **Organisers:**
  [Matthew Farrugia-Roberts](https://far.in.net) and Dan Murfet.
* **Time:**
  Thursday evenings, 9pm AEDT (UTC +11), most weeks
  (see [home page](/) for most up-to-date schedule).
* **Venue:**
  [The Rising Sea](https://www.roblox.com/games/8165217582/The-Rising-Sea).

Directions for joining discussions:

0. New to metauni?
   Follow [these instructions (part 2)](/posts/instructions/instructions)
   to join the metauni Discord server, and introduce yourself in the channel
   `#ai-safety`.
1. Metauni talks take place in Roblox using in-game voice chat.
   Follow [these instructions (part 1)](/posts/instructions/instructions)
   to create a Roblox account, complete "age verification" (unfortunately,
   this involves sharing ID with Roblox), and then enable Roblox "voice chat".
2. At the scheduled discussion time, launch the Roblox experience
   [The Rising Sea](https://www.roblox.com/games/8165217582/The-Rising-Sea)
   and then step into matomatical's portal (bottom-right corner of stack, see
   picture),
   or use the menu: "Pockets" > "Go to pocket" > type address "Gemini Pulsar 1".
   ![](map.jpg)

Readings
--------

Completing weekly readings is recommended, but ultimately optional.
The discussion sessions begin with a summary of the reading, led by Matt
(unless otherwise noted).

Upcoming readings and discussions:

* **TBD (2023):**
  Guest speaker: Tom Everitt
  (Location: Discord)

Past readings and discussions:

* **2022.06.09:**
  Norbert Wiener,
  1960,
  "Some moral and technical consequences of automation",
  *Science*.

* **2022.06.16:**
  Stephen M. Omohundro,
  2008,
  "The basic AI drives",
  *Proceedings of the 2008 conference on Artificial General Intelligence*.

* **2022.06.23:**
  Nick Bostrom,
  2012,
  "The superintelligent will: Motivation and instrumental rationality in
  advanced artificial agents",
  *Minds and Machines*.

* **2022.06.30:**
  Rachel Thomas and Louisa Bartolo,
  2022,
  "AI harms are societal, not just individual",
  [fast.ai blog](https://www.fast.ai/2022/05/17/societal-harms/).
  Discussion led by Dan.

* **2022.07.21:**
  Tobias Wängberg *et al.*,
  2017,
  "A game-theoretic analysis of the of the off-switch game",
  *AGI 2017*.

* **2022.07.28:**
  Abram Demski and Scott Garrabrant,
  2019,
  "Embedded agency",
  [arXiv](https://arxiv.org/abs/1902.09469)
  / [sequence](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh).

* **2022.08.04:**
  Scott Garrabrant *et al.*,
  2017,
  "Logical induction",
  [arXiv](https://arxiv.org/abs/1609.03543v4).
  Discussion led by Dan.
  Note: there is an updated 2020 version on arXiv.

* **2022.08.11:**
  Robin Hanson,
  2022,
  "Why not wait on AI risk?",
  [overcoming bias blog](https://www.overcomingbias.com/2022/06/why-not-wait-on-ai-risk.html)
  and
  "Foom update",
  [overcoming bias blog](https://www.overcomingbias.com/2022/05/foom-update.html).

* **2022.08.18:**
  Evan Hubinger *et al.*,
  2019,
  "Risks from learned optimization"
  [arXiv](https://arxiv.org/abs/1906.01820)
  / [sequence](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB).

* **2022.08.25:**
  an original presentation by Matt about compression and learning in models
  of computation embedded in the real world.

* **2022.09.08:**
  discussion of reading group direction.

* **2022.09.15:**
  Nate Soares,
  2022,
  "On how various plans miss the hard bits of the alignment challenge",
  [lesswrong](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment).

* **2022.11.24:**
  Guest speaker: Elliot Catt.
  An overview of mathematical approaches to AGI safety.

* **2022.12.01:**
  Eliezer Yudkowsky,
  2013,
  "Intelligence explosion microeconomics",
  MIRI [technical report](https://intelligence.org/files/IEM.pdf).

* **2022.12.15:**
  Scott Aaronson,
  2022,
  "Reform AI Alignment",
  [*Shtetl-Optimized* blog](https://scottaaronson.blog/?p=6821).
  Also,
  Boaz Barak and Ben Edelman,
  2022,
  "AI will change the world, but won't take it over by playing '3-dimensional
  chess'",
  [*Windows On Theory* blog](https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/).
  Discussion led by Dan.


Topics brainstorm
-----------------

AI safety is political philosophy complete:

* Exernalities correspond to market alignment failures. How does society
  such alignment failures? (How) does society overcome them? Does society
  face risk from them? Would such risks be exacerbated by AI progress?
* How can we live in the midst of complex systems we don't understand, and
  can't fully control, like civilisation, capitalism, etc.?
* What other literatures could help us here?

Other topics

* AI governance
* Is there literature on technology and society?
* Specific safety proposals

Sources of readings (clearly with much mutual overlap):

* Matt's lists (TODO: share them).
* Victoria Krakovna's
  [resource](https://vkrakovna.wordpress.com/2016/02/28/introductory-resources-on-ai-safety-research/)
  [lists](https://vkrakovna.wordpress.com/ai-safety-resources/).
* Rohin Shah's
  [2018/2019 review](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review).
* CHAI AI safety [bibliography](https://humancompatible.ai/bibliography)
* Publications from
  [MIRI](https://intelligence.org/research/#publications),
  [FHI](http://www.fhi.ox.ac.uk/publications/), etc.
* The old 80kh AI safety
  [syllabus](https://80000hours.org/articles/ai-safety-syllabus/)
  and links therein (esp. EA Cambridge syllabus).
