---
title:
    metauni Alignment Project
description:
    There are no algebraists in foxholes
---

If aligning an AGI is possible, it **must be because of affordances due to universality**. We're talking about controlling a system at scales that range from sub-human intelligence to vastly superhuman intelligence, and the key problem is how to engineer solutions that remain effective across this wide range of scales. The shadow of this problem can be seen in the "sharp left turn". This may be possible if enough critical components of the system being aligned are *universal*.

## Universal means interpretable

## We can develop a theory of universality in learning machines

Notes

- Intentions may not be interpretable (the *why*), but the *how* might be
- In some limit, trajectories interacting with noise behave like particles in a supersymmetric field theory. Then to study transitions you want to group trajectories into kinds which involve rep theory
- systems coupled to noise see the jet scheme

- Paradox of safety

## Know your enemy, know yourself

## Spectroscopy at Scale

- automated discovery of band structure

## Concepts as Components

## Programs as Processes

- See more data -> simple model breaks. But how does it break?

## Universality and CFT
