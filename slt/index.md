---
title:
    Singular Learning Theory seminar
description:
    Algebraic Geometry & Statistical Learning Theory
---

This is the homepage of a seminar on Singular Learning Theory (SLT), a theory applying algebraic geometry to statistical learning theory founded by [Sumio Watanabe](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/). The seminar takes place at [metauni](https://www.metauni.org), for the schedule see the metauni homepage.

![slt-pic-min](https://user-images.githubusercontent.com/320329/208009611-93289eb6-4813-436d-864e-4deb487a8b73.png)

The canonical references are Watanabe's two textbooks:

* **The gray book:** S. Watanabe "[Algebraic geometry and statistical learning theory](https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A)" 2009.
* **The green book:** S. Watanabe "[Mathematical theory of Bayesian statistics](https://www.routledge.com/Mathematical-Theory-of-Bayesian-Statistics/Watanabe/p/book/9780367734817)" 2018.

Some other introductory references:

* Matt Farrugia-Roberts' MSc thesis, October 2022, [Structural Degeneracy in Neural Networks](https://far.in.net/mthesis).
* Spencer Wong's MSc thesis, May 2022, [From Analytic to Algebraic: The Algebraic Geometry of Two Layer Neural Networks](http://therisingsea.org/notes/MScThesisSpencerWong.pdf).
* [Liam Carroll](https://lemmykc.github.io/MDLG_lemmykc/)'s MSc thesis, October 2021, [Phase transitions in neural networks](http://therisingsea.org/notes/MSc-Carroll.pdf).
* Tom Waring's MSc thesis, October 2021, [Geometric Perspectives on Program Synthesis and Semantics](http://therisingsea.org/notes/MSc-Waring.pdf).
* S. Wei, D. Murfet, M. Gong, H. Li , J. Gell-Redman, T. Quella "[Deep learning is singular, and that's good](https://www.suswei.com/publication/wei-2022-singular/wei-2022-singular.pdf)" 2022.
* Edmund Lau's blog [Probably Singular](https://edmundlth.github.io/posts/singular-learning-theory-part-1/).
* Shaowei Lin's PhD thesis, 2011, [Algebraic Methods for Evaluating Integrals in Bayesian Statistics](https://escholarship.org/content/qt6r99035v/qt6r99035v_noSplash_55ad6962455379ca776283fed8278b40.pdf).
* Jesse Hoogland's blog posts: [general intro to SLT](https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick), and [effects of singularities on dynamics](https://www.lesswrong.com/posts/2N7eEKDuL5sHQou3N/spooky-action-at-a-distance-in-the-loss-landscape).

## Upcoming seminars

* **9-2-23** (*Dan Murfet*): Solid state physics and SLT Pt 1
* **16-2-23** (*Dan Murfet*): Solid state physics and SLT Pt 2
* **23-2-23** (*Edmund Lau*): Occam's razor following Balasubramanian 
* **TBD** (*Russell Goyder*)
* **TBD** (*Rohan Hitchcock*)

Some topics and papers to be discussed:

* "[Scaling laws for reward model overoptimization](https://arxiv.org/abs/2210.10760)" L. Gao, J. Schulman, J. Hilton (played a role in the creation of ChatGPT).
* "Statistical inference, Occam's razor, and statistical mechanics on the space of probability distributions" V. Balasubramanian.
* "[Toy models of superposition](https://transformer-circuits.pub/2022/toy_model/index.html)" Anthropic.
* "[A mathematical theory of semantic development in deep neural networks](https://www.pnas.org/doi/epdf/10.1073/pnas.1820226116)" A. Saxe, J. McClelland, S. Ganguli 2019.
* See this [survey of scaling laws](https://epochai.org/blog/scaling-laws-literature-review).

## Previous seminars

### 2023

* **19-1-23** (*Russell Goyder*): Physical entropy vs information-theoretic entropy Pt 2 ([video](https://youtu.be/rw23vmz2MF4), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2020), [transcript](https://metauniservice.com/transcript?videoID=rw23vmz2MF4))
* **26-1-23** (*Dan Murfet*): Towards in-context learning in SLT Pt 1 ([video](https://youtu.be/TuFPGNBoOuM), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2011), [transcript](https://metauniservice.com/transcript?videoID=TuFPGNBoOuM))
* **2-2-23** (*Dan Murfet*): Towards in-context learning in SLT Pt 2 ([video](https://youtu.be/LG7b587q8T0), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2011))

### 2022

Below you can find the seminars for 2022, with videos and pocket links (which take you to the virtual world where the talk took place, with the blackboards just as we left them at the end of the talk).

* **13-1-22** (*Dan Murfet*): What is learning? Singularities and pendulums ([video](https://youtu.be/QZG40ZY5TeU), [transcript](https://metauniservice.com/transcript?videoID=QZG40ZY5TeU)).
* **13-1-22** (*Edmund Lau*): The Fisher information matrix ([video](https://youtu.be/yniLt7ONj28), [transcript](https://metauniservice.com/transcript?videoID=yniLt7ONj28)).
* **20-1-22** (*Edmund Lau*):  Fisher information, KL-divergence and singular models ([video](https://youtu.be/U9bnkWuFSSM)).
* **20-1-22** (*Liam Carroll*): Markov Chain Monte Carlo ([video](https://youtu.be/Ns4w0vtWt4A)).
* **27-1-22** (*Liam Carroll*): Neural networks and the Bayesian posterior ([video](https://youtu.be/1Esk7G3g5X8))
* **27-1-22** (*Spencer Wong*): Rings, ideals and the Hilbert basis theorem ([video](https://youtu.be/g1tXe9Yrij8)).
* **3-2-22** (*Spencer Wong*): From analytic to algebraic I ([video](https://youtu.be/5Gkzg-zTwv4)).
* **3-2-22** (*Ken Chan*): Resolution of singularities ([video](https://youtu.be/ssU8VZ50Wd8)).
* **10-2-22** (*Dan Murfet*): Introduction to density of states ([video](https://youtu.be/HXCpQWZfWIw), [notes](http://www.therisingsea.org/notes/metauni/slt12.pdf), [transcript](https://metauniservice.com/transcript?videoID=HXCpQWZfWIw)).
* **10-2-22** (*Spencer Wong*): Polynomial division ([video](https://youtu.be/nNMCix6UCJ0)).
* **17-2-22** (*Spencer Wong*): From analytic to algebraic II ([video](https://youtu.be/tsZjeclrmuU)).
* **17-2-22**: Working session 1 ([video](https://youtu.be/cuPeJkeiYsI)).
* **24-2-22** (*Edmund Lau*): Free energy asymptotics ([video](https://youtu.be/QBaJH5QRAA8))
* **24-2-22**: Working session 2 ([video](https://youtu.be/c7Di-oAZxNg))
* **3-3-22** (*Spencer Wong*): From analytic to algebraic III ([video](https://youtu.be/LWylEE5M9lc)).
* **3-3-22**: Working session 3 ([video](https://youtu.be/kqP5I2wALt0)).
* **10-3-22** (*Tom Waring*): Regularly parametrised models ([video](https://youtu.be/T8Lgvt0mfuY)).
* **17-3-22** (*Edmund Lau*): Bounding the partition function ([video](https://youtu.be/7x16e4yHsHg)).
* **24-3-22** (*Edmund Lau*): The influence of sampling ([video](https://youtu.be/JyZnMinS86Q)).
* **7-4-22** (*Edmund Lau*): Main Theorem 1 ([video](https://youtu.be/70UtL7pfxNo)).
* **14-4-22** (*Edmund Lau*): Main Theorem 2 ([video](https://youtu.be/qE3v1044BwU)).
* **8-9-22** (*Matt Farrugia-Roberts*): Complexity of rank estimation ([video](https://youtu.be/s2bgR_t3aGM), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%205)).
* **15-9-22** (*Matt Farrugia-Roberts*): Piecewise-linear paths in equivalent networks ([video](https://youtu.be/WBiFFIhGIZM), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%205)).
* **22-9-22** (*various*) A minimal introduction to the geometry of tanh networks ([video](https://youtu.be/EgqwUsJTumU), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%205)).
* **29-9-22** (*Dan Murfet*): Information theory I - entropy and KL divergence ([video](https://youtu.be/LDAtVVPazg4), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%207), [transcript](https://metauniservice.com/transcript?videoID=LDAtVVPazg4)).
* **6-10-22** (*Zhongtian Chen*): The Kraft-McMillan theorem ([video](https://youtu.be/N30wJhaO68k), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ABig%20Sir%202)).
* **13-10-22** (*Edmund Lau*): Asymptotic learning curve and renormalizable condition in statistical learning theory ([video](https://youtu.be/FhNoHmxmCmg), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ABig%20Sir%202)).
* **13-10-22** (*Dan Murfet*): Intro to blowing up (cross-posted from the [Abstraction seminar](https://metauni.org/abstraction/), [video](https://youtu.be/CWNaKMP8Teo), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ABig%20Sir%201)).
* **20-10-22** (*Dan Murfet*): State of scaling laws 2022 ([video](https://youtu.be/7LzW8-wxdUE), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2010), [transcript](https://metauniservice.com/transcript?videoID=7LzW8-wxdUE)).
* **27-10-22** (*Dan Murfet*): In-context learning ([video](https://youtu.be/Vqmcn1q7VL0), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2011), [transcript](https://metauniservice.com/transcript?videoID=Vqmcn1q7VL0)).
* **3-11-22** (*Dan Murfet*): Open problems ([video](https://youtu.be/ZdB0pWyPaFo), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2012), [transcript](https://metauniservice.com/transcript?videoID=ZdB0pWyPaFo)).
* **10-11-22** (*Edmund Lau*): Newton diagrams in singular learning theory ([video](https://youtu.be/0FowiD36jwg), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2013)).
* **17-11-22** (*Matt Farrugia-Roberts*): Overview of MSc thesis ([video](https://youtu.be/KwI-O8VKVv4), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2015)).
* **24-11-22** (*Dan Murfet*): Jet schemes I ([video](https://youtu.be/8g6dZZ7lWGk), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2016)).
* **1-12-22** (*Matt Farrugia-Roberts*): Overview of MSc thesis Pt 2 ([video](https://youtu.be/eBdm1pPGsco), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket:Symbolic%20Wilds%2016)).
* **8-12-22** (*Dan Murfet*): Jet schemes II ([video](https://youtu.be/kHfDyK43HKc), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2016)).
* **15-12-22** (*Matt Farrugia-Roberts*): Overview of MSc thesis Pt 3 ([video](https://youtu.be/FngNdKxCIuY), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2017)).
* **22-12-22** (*Russell Goyder*) Physical entropy vs information-theoretic entropy ([video](https://youtu.be/MFLQgCaijPk), [pocket](https://www.roblox.com/games/start?placeId=8165217582&launchData=pocket%3ASymbolic%20Wilds%2020)).

![shot](https://user-images.githubusercontent.com/320329/200425686-62de6535-d334-4721-a6cb-712568a48b6a.png)

## Background

* A. Karpathy on [Transformers](https://youtu.be/kCc8FmEb1nY) (on [data distribution](https://youtu.be/kCc8FmEb1nY?t=867)).

Some rough handwritten notes:

* [Deep Learning Theory 1](http://www.therisingsea.org/notes/metauni/dlt1.pdf): Why deep learning theory?
* [Deep Learning Theory 2](http://www.therisingsea.org/notes/metauni/dlt2.pdf): Thermodynamics of Singular Learning Theory
* [Deep Learning Theory 3](http://www.therisingsea.org/notes/metauni/dlt3.pdf): Phase transitions
* [Singular Learning Theory 4](http://www.therisingsea.org/notes/metauni/slt4.pdf): Local RLCT
* [Singular Learning Theory 5](http://www.therisingsea.org/notes/metauni/slt5.pdf): Symmetry and RLCT
* [Singular Learning Theory 6](http://www.therisingsea.org/notes/metauni/slt6.pdf): Generalisation and Power Laws
* [Singular Learning Theory 8](http://www.therisingsea.org/notes/metauni/slt8.pdf): Calculations for feedforward networks
* [Singular Learning Theory 12](http://www.therisingsea.org/notes/metauni/slt12.pdf): Density of states
* [Singular Learning Theory 13](http://www.therisingsea.org/notes/metauni/slt13.pdf): Asymptotics of the free energy

## FAQ

**Question:** Is SLT relevant to AI alignment?

**Answer:** *tldr* Yes, but also relevant to capabilities. *Caveat emptor*.

(*Dan Murfet*'s personal views here) First some caveats: although we are optimistic SLT can be developed into theory of deep learning, it is not currently such a theory and it remains possible that there are fundamental obstacles. Putting those aside for a moment, it is plausible that phenomena like scaling laws and the related emergence of capabilities like in-context learning can be understood from first principles within a framework like SLT. This could contribute both to capabilities research and safety research.

*Contribution to capabilities.* Right now it is not understood why Transformers obey scaling laws, and how capabilities like in-context learning relate to scaling in the loss; improved theoretical understanding could increase scaling exponents or allow them to be engineered in smaller systems. For example, some empirical work already suggests that certain data distributions lead to in-context learning. It is possible that theoretical work could inspire new ideas. Thermodynamics wasn't necessary to build steam engines, but it *helped to push the technology to new levels of capability* once the systems became too big and complex for tinkering.

*Contribution to alignment.* Broadly speaking it is hard to align what you do not understand. Either the aspects of intelligence relevant for alignment are universal, or they are not. If they are not, we have to get lucky (and stay lucky as the systems scale). If the relevant aspects are universal (in the sense that they arise for fundamental reasons in sufficiently intelligent systems across multiple different substrates) we can try to understand them and use them to control/align advanced systems (or at least be forewarned about their dangers) and be reasonably certain that the phenomena continue to behave as predicted across scales. This is one motivation behind the work on properties of optimal agents, such as Logical Inductors. SLT is a theory of universal aspects of learning machines, it could perhaps be developed in similar directions.

*Does understanding scaling laws contribute to safety?*. It depends on what is causing scaling laws. If, as we suspect, it is about phases and phase transitions then it is related to the nature of the structures that emerge during training which are responsible for these phase transitions (e.g. concepts). A theory of interpretability scalable enough to align advanced systems may need to develop a fundamental theory of abstractions, especially if these are related to the phenomena around scaling laws and emergent capabilities.

Our take on this has been partly spelled out in the [Abstraction seminar](https://metauni.org/abstraction/). We're trying to develop existing links in mathematical physics between renormalisation group flow and resolution of singularities, which applied in the context of SLT might give a fundamental understanding of how abstractions emerge in learning machines. One best case scenario of the application of SLT to alignment is that this line of research gives a theoretical framework in which to understand more empirical interpretability work.

The utility of theory in general and SLT in particular depends on your mental model of the problem landscape between here and AGI. To return to the thermodynamics analogy: a working theory of thermodynamics isn't necessary to build train engines, but it's probably necessary to build rockets. If you think the engineering-driven approach that has driven deep learning so far will plateau before AGI, probably theory research is bad in expected value. If you think theory isn't necessary to get to AGI, then it may be a risk that we have to take.

Summary: In my view we seem to know enough to get to AGI. We do not know enough to get to alignment. Ergo we have to take some risks.
